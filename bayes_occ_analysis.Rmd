---
title: "03a_ch1_analysis_new"
author: "Kirsten Fuller"
date: "1/18/2022"
output: html_document
---
# Introduction

## Set Working Directory
```{r setup, include=FALSE}
rm( list = ls() ); gc( )

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Documents/Boise_State/thesis/ms_thesis_ch1")
```

## Prep Workspace
```{r, include = FALSE, message=FALSE}
# load necessary packages
# define packages
packages <- c("coda", "rjags", "dplyr", "tidyr", "jagsUI", "ggmcmc", "sf") # jagsUI = JAGS user interface

# install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# load packages
invisible(lapply(packages, library, character.only = TRUE))

# set options related to packages
options( dplyr.width = Inf, dplyr.print_min = 100 ) # see all columns and more than 10 rows
```

# Basic analysis steps:
1. Collect and package data
2. Write a model file in BUGS language
3. Set initial values
4. Specify parameters to monitor
5. Set MCMC variables and run analysis
6. Optionally, generate more posterior samples using the update method

## Load Data
```{r}
hooting.df <- read.csv("data/output/ch1_hooting_data")

# we technically had 4 surveys outside of the survey period, let's eliminate those
hooting.df <- hooting.df %>%
  filter(julian_date < 244) %>%
  filter(julian_date > 62)
```

## Define Data
Create separate data frames for site level data and survey level data and standardize the predictor values.
```{r}
# SURVEYS:
# define the primary seasons (min to max)
yrs <- min(hooting.df$survey_year):max(hooting.df$survey_year)
yrs  # this includes years when no sampling occurred

# define total length of data period
K <- length(yrs)
K # 21 years


# SITES:
# define the number of sites
I <- length(unique(hooting.df$pac_surveyed))
I # 53 sites

# define the max number of replicate surveys
J <- max(hooting.df$survey_number)
J # 6 is the max number of replicate surveys

# define the number of surveys conducted
T <- nrow(hooting.df)
T # 306 surveys conducted

# add site (pac) number as a new column to the hooting data frame
hooting.df$siteid <- as.numeric(as.factor(hooting.df$pac_surveyed)) # JAGS will expect the sites to be numbered


# MAKE COVARIATE DATAFRAMES
# extract site level covariates of interest:
X <- hooting.df %>% # define df  
  dplyr::group_by(siteid) %>% # group by siteid
  slice(1) %>% # select only first row since other values are replicates
  dplyr::select(COLEOGYNE, PJCON, MSHRUB, canyon_geometry, people, supai, redwall, siteid) 

# standardize predictors
X[,1:7] <- apply( X[,1:7], MARGIN = 2, FUN = scale )

# view
head(X); dim(X)

# extract survey level covariates
XJ <- hooting.df %>% 
  mutate(yearid = as.numeric(as.factor(survey_year))) %>%
  dplyr::select(pac_area, julian_date, siteid, yearid, surveyid = survey_number, y_obs = presence)

# standardize covariates of interest
XJ[,1:2] <- apply( XJ[,1:2], MARGIN = 2, FUN = scale )

# view
head(XJ);dim(XJ)
```

## Define MCMC Sampling Settings
```{r}
nt <- 5 # thinning = a measure of how much the MCMC chains should be thinned out before storing them. Default is 1. A value of 5 means that we keep every 5th value and discard the others.
nb <- 10000 # burning = the chains we get rid of at the beginning of an mcmc run (10,000 seems high) before it converges; is this the same as iterations?
nc <- 5 # chains = the number of chains, default is 4 I believe
```

  
## Write Model
Multi-season occupancy model (different from dynamic because it excludes extinction and colonization parameters).
```{r}
sink( "dm3.txt" )
cat( "
     model{
     
      #priors
      # define intercept for occupancy
      # note that this is using the logit transformation
      
  # occupancy probability
      int.psi <- log( mean.psi / ( 1 - mean.psi ) )
      mean.psi ~ dbeta( 4, 4 )  
      
# it allows us to set the prior on the real scale where the intercept represent the mean probability averaged across sites and years...here we use a beta distribution which ranges between 0 to 1, the parameters given to this prior should result in a tight, bell-shaped distribution centered around 0.5 - plot different distributions to see what they look like.
      
# This same process is used to assign our prior for the intercept in the detection model
      
  # detection probability
      int.p <- log( mean.p / ( 1 - mean.p ) )
      mean.p ~ dbeta( 4, 4 )
     
  # random year intercepts
      for ( k in 1:K ){ 
        eps.k[ k ] ~ dnorm( 0, prec.k ) T(-7, 7) # random year intercept for ecological model
        eps.p[ k ] ~ dnorm( 0, prec.p ) T(-7, 7) # random year intercept for observation model
      } # K
      
  # random site intercept 
      for ( i in 1:I ){
        eps.i[ i ] ~ dnorm( 0, prec.i ) T(-7, 7) # random site intercept for ecological model
       } # i
      
 # error terms for random intercepts for detection model:
      # associated precision of random intercepts for detection is estimated by 1/variance 
      prec.p <- 1 / ( sigma.p * sigma.p )
      sigma.p ~ dt( 0, 2.5, 7 ) T( 0, )
      
 # error terms for the random intercepts for ecological model:
      # random intercept for year
      prec.k <- 1 / ( sigma.k * sigma.k )
      sigma.k ~ dt( 0, 2.5, 7 ) T( 0, )
      
      # random intercept for site
      prec.i <- 1 / ( sigma.i * sigma.i )
      sigma.i ~ dt( 0, 2.5, 7 ) T( 0, )
    
# priors for fixed effects (coefficients) in ecological model:
      for( q in 1:7 ){ # loop over number of predictors
        beta.psi[ q ] ~ dnorm( 0, 0.1 ) # this precision is not fully uninformative but actually regularizes the estimates around 0 to help computation
      }
      
# priors for fixed coefficients in detection submodel:
      for( q in 1:2 ){ # loop over number of predictors
        beta.p[ q ] ~ dnorm( 0, 0.1 ) # prior for detection predictors
      
      } # Q
# END OF PRIORS 
    
# occupancy model:
        for( i in 1:I ){  # loop over sites
          for ( k in 1:K ) { # loop over years
            # true occupancy state, z, is given a bernoulli distribution with probability psi:
            z[ i, k ] ~ dbern( psi[ i, k ] ) 
              
              # probability of occupancy, psi is linked to predictors using a logit function:
              logit( psi[ i, k ] ) <- int.psi + 
                  # random intercepts:
                  eps.k[ k ] + 
                  eps.i[ i ] +
                  
                  # fixed effects:
                  beta.psi[ 1 ] * X[ i, 5 ] + # people
                  beta.psi[ 2 ] * X[ i, 1 ] + # blackbrush
                  beta.psi[ 3 ] * X[ i, 2 ] + # pjcon
                  beta.psi[ 4 ] * X[ i, 3 ] + # mshrub
                  beta.psi[ 5 ] * X[ i, 6 ] + # supai
                  beta.psi[ 6 ] * X[ i, 7 ] + # redwall
                  beta.psi[ 7 ] * X[ i, 4 ]   # canyon geometry
          } # K years
        } # I sites
    

# detection model:
      for( t in 1:T ){  # loop over all replicates
      
        # we link probability of detection, p, to predictors using a logit function
        logit( p[ t ] ) <- int.p +
            # random intercept:
            eps.p[ yearid[t] ] +
      
            # fixed effects:
            beta.p[ 1 ] * pac_area[ t ] + # area
            beta.p[ 2 ] * julian_date[ t ] # julian day 
             
# Here we link our observations to the estimated, true occupancy, z, from our ecological model above
        y_obs[ t ] ~ dbern( z[ siteid[t], yearid[t] ] * p[ t ] ) 
             
             
# For Model Evaluation:     

# Here we estimate  what the model would have produced as observations.
      
        yhat[ t ] ~ dbern( z[ siteid[t], yearid[t] ] * p[ t ] ) 
        
# To finish, we also estimate the likelihood of observed and predicted values
    
    # likelihood of observations
        lik_yobs[ t ] <- ( ( psi[ siteid[t], yearid[t] ] * p[ t ] )^y_obs[ t ] ) *
              ( ( 1 - psi[ siteid[t], yearid[t] ] * p[ t ] )^( 1 - y_obs[ t ] ) )
              
    # likelihood of estimated detections:
        lik_yhat[ t ] <- ( ( psi[ siteid[t], yearid[t] ]* p[ t ] )^yhat[ t ] ) *
            ( ( 1 - psi[ siteid[t], yearid[t] ] * p[ t ] )^( 1 - yhat[ t ] ) )
      } # t
     } # model close
     
     ", fill = TRUE )

sink()
```

## Run Model 3
```{r}
# name the model
modelname <- "dm3.txt"

# parameters monitored - only keep those relevant for model comparisons
params <- c('int.psi' # intercept for occupancy model
            ,'int.p' # intercept for detection
            ,'beta.psi' # fixed coefficients for occupancy
            ,'beta.p' # fixed coefficients for detection
            ,'eps.k' # random year intercept for occupancy 
            ,'sigma.k' # standard deviation associated with random year for occupancy
            ,'eps.p' # random year intercept for detection 
            ,'sigma.p' # standard deviation associated with random year for detection
            ,'eps.i' # random site intercept for occupancy
            ,'sigma.i' # standard deviation associated with detection
            ,'z' # estimated occupancy state
            ,'psi' # probability of occupancy
            ,'p' # probability of detection
            ,'lik_yobs' # likelihood for each occupancy observation
            ,'lik_yhat' # likelihood for occupancy observations predicted by the model
            ,'yhat') # estimated occurrence from model

# create initial values for the model coefficients
zst <- matrix(data = 1, nrow = I, ncol = K )
# pst <- array(data = 1, dim = c(I,K,J) )

# create initial values to start the algorithm
inits <- function(){ list( beta.psi = rnorm( 7 ),
                           beta.p = rnorm( 2 ), 
                           z = zst)
  }
# combine data into object:
str( win.data <- list( y_obs = XJ$y_obs, # observed occupancy for each species
                       K = K , J = J, I = I, T = T,
                       # site level predictors
                       X = as.matrix(X[,1:7]),
                       # survey level predictors
                       siteid = XJ$siteid,
                       yearid = XJ$yearid,
                       surveyid = XJ$surveyid,
                       julian_date = XJ$julian_date,
                       pac_area = XJ$pac_area))                

# call JAGS and summarize posteriors:
m3 <-  autojags( win.data, inits = inits, params, modelname,
                 n.chains = nc, n.thin = nt, n.burnin = nb,
                 iter.increment = 20000, max.iter = 500000, 
                 Rhat.limit = 1.02, 
                 save.all.iter = FALSE, parallel = TRUE ) 

print(m3, dig = 2)

# posterior predictive plot check for model fit
# pp.check(m3, observed = 'yhat', simulated = 'lik_yhat')
# this is based off of the discrepancy metrics calculated for the real data (for example, the sum of residuals)
```

## Save Model Results
```{r}
# save the workspace
save.image("data/output/bayes_occ_results.RData")

# save the best model
saveRDS(m3, file = "~/Documents/Boise_State/thesis/ms_thesis_ch1/data/output/model_3_results.rds")
```


